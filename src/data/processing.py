""" data 정제 및 convert module. 다양한 source data process하고 analysis 위한 준비 numberrows. """ import os import pandas as pd import numpy as np import logging import yaml from datetime import datetime from pathlib import Path from typing import Dict, List, Optional, Union, Any, Callable logger = logging.getLogger(__name__) class DataProcessor: """data process 및 convert 위한 class.""" def __init__(self, config_path: str = "config/pipeline_config.yaml"): """ DataProcessor initialize. Parameters ---------- config_path: str configuration file 경 """ self.config_path = config_path self.config = self._load_config() def _load_config(self) -> Dict[str, Any]: """ load configuration file. Returns ------- Dict[str, Any] dictionary containing configuration information """ try: with open(self.config_path, 'r', encoding='utf-8') as f: config = yaml.safe_load(f) return config except Exception as e: logger.error(f"configuration file load failed: {e}") raise def clean_dataframe(self, df: pd.DataFrame, drop_duplicates: Optional[bool] = None, handle_missing: Optional[str] = None, drop_na_thresh: Optional[int] = None, na_columns: Optional[List[str]] = None) -> pd.DataFrame: """ DataFrame 정제. Parameters ---------- df: pd.DataFrame 정제할 DataFrame drop_duplicates: bool, optional 중복 rows 제거 여부 (configuration file value 우선) handle_missing: str, optional 결측치 process method ('drop', 'fill_mean', 'fill_median', 'fill_zero') drop_na_thresh: int, optional 결측치 value 상인 rows 제거 na_columns: list of str, optional 결측치 check할 columns 목록 Returns ------- pd.DataFrame 정제된 DataFrame """ if df.empty: logger.warning("정제할 data not available.") return df # configuration file value usage if drop_duplicates is None: drop_duplicates = self.config['processing'].get('clean_duplicates', True) if handle_missing is None: handle_missing = self.config['processing'].get('handle_missing_values', 'drop') # 원본 data 복사 cleaned_df = df.copy() original_shape = cleaned_df.shape # 중복 제거 if drop_duplicates: cleaned_df = cleaned_df.drop_duplicates() if original_shape[0] > cleaned_df.shape[0]: logger.info(f"중복 rows 제거됨: {original_shape[0] - cleaned_df.shape[0]}개") # 결측치 process if handle_missing == 'drop': if na_columns: cleaned_df = cleaned_df.dropna(subset=na_columns) else: if drop_na_thresh: cleaned_df = cleaned_df.dropna(thresh=drop_na_thresh) else: cleaned_df = cleaned_df.dropna() if original_shape[0] > cleaned_df.shape[0]: logger.info(f"결측치 rows 제거됨: {original_shape[0] - cleaned_df.shape[0]}개") elif handle_missing == 'fill_mean': # number형 columns만 평균 apply numeric_cols = cleaned_df.select_dtypes(include=['number']).columns for col in numeric_cols: cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean()) logger.info(f"number형 columns 결측치 평균value 대체: {len(numeric_cols)}개 columns") elif handle_missing == 'fill_median': # number형 columns만 중앙value apply numeric_cols = cleaned_df.select_dtypes(include=['number']).columns for col in numeric_cols: cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median()) logger.info(f"number형 columns 결측치 중앙value 대체: {len(numeric_cols)}개 columns") elif handle_missing == 'fill_zero': # all columns 0 apply cleaned_df = cleaned_df.fillna(0) logger.info("all 결측치 0 대체") logger.debug(f"data 정제 completed, 원본: {original_shape}, 정제 후: {cleaned_df.shape}") return cleaned_df def standardize_date_format(self, df: pd.DataFrame, date_cols: Optional[List[str]] = None) -> pd.DataFrame: """ date format 표준화. Parameters ---------- df: pd.DataFrame process할 DataFrame date_cols: list of str, optional date columns 목록. None면 자동 감지 Returns ------- pd.DataFrame date 표준화된 DataFrame """ if df.empty: return df # 원본 data 복사 processed_df = df.copy() # configurationfrom date format get date_format = self.config['processing'].get('standardize_date_format', 'YYYY-MM-DD') # pandas format convert if date_format == 'YYYY-MM-DD': pd_format = '%Y-%m-%d' elif date_format == 'MM/DD/YYYY': pd_format = '%m/%d/%Y' elif date_format == 'DD/MM/YYYY': pd_format = '%d/%m/%Y' else: pd_format = '%Y-%m-%d' # default value # date columns 자동 감지 if date_cols is None: # columns name 'date', 'day', 'month', 'year' 등 include된 columns 찾기 possible_date_cols = [col for col in processed_df.columns if any(key in col.lower() for key in ['date', 'day', 'month', 'year'])] # data 타입 object, string, datetime인 columnsfrom만 date convert 시도 date_cols = [] for col in possible_date_cols: if col in processed_df.columns and processed_df[col].dtype in ['object', 'string', 'datetime64[ns]']: try: # 샘플 value date convert 테스트 sample = processed_df[col].dropna().iloc[0] if not processed_df[col].dropna().empty else None if sample and isinstance(sample, str): pd.to_datetime(sample) date_cols.append(col) except: # convert failed 시 무시 pass # date columns process for col in date_cols: if col in processed_df.columns: try: # datetime convert processed_df[col] = pd.to_datetime(processed_df[col], errors='coerce') # 지정된 format 다시 string convert processed_df[col] = processed_df[col].dt.strftime(pd_format) logger.info(f"date columns '{col}' format 표준화 completed") except Exception as e: logger.warning(f"date columns '{col}' format 표준화 failed: {e}") return processed_df def merge_dataframes(self, dataframes: Dict[str, pd.DataFrame], merge_keys: Optional[List[str]] = None, how: str = 'left') -> pd.DataFrame: """ 여러 DataFrame 병합. Parameters ---------- dataframes: Dict[str, pd.DataFrame] source별 DataFrame dictionary merge_keys: list of str, optional 병합 usage할 키 목록 how: str, default='left' 병합 method ('left', 'right', 'inner', 'outer') Returns ------- pd.DataFrame 병합된 DataFrame """ if not dataframes: logger.warning("병합할 DataFrame not available.") return pd.DataFrame() # configurationfrom 병합 키 get if merge_keys is None: merge_keys = self.config['processing'].get('merge_keys', ['date']) # source 1개면 그대 return if len(dataframes) == 1: source_name, df = next(iter(dataframes.items())) logger.info(f"DataFrame 1개뿐므 병합 없 return: {source_name}, size: {df.shape}") return df try: # 첫 번째 DataFrame 기준 usage sources = list(dataframes.keys()) result_df = dataframes[sources[0]].copy() logger.info(f"병합 기준 DataFrame: {sources[0]}, size: {result_df.shape}") # 나머지 DataFrame 병합 for i in range(1, len(sources)): source = sources[i] df = dataframes[source] # 병합 키 DataFrame 있지 check common_keys = [key for key in merge_keys if key in result_df.columns and key in df.columns] if not common_keys: logger.warning(f"'{source}' DataFrameand 병합할 공통 키 not available. 병합 건너뜁니다.") continue # source 구분 위한 접두사 result_prefix = f"{sources[0]}_" curr_prefix = f"{source}_" # 중복 columns process 위해 접두사 add result_df = pd.merge( result_df, df, on=common_keys, how=how, suffixes=(f"_{sources[0]}", f"_{source}") ) logger.info(f"'{source}' DataFrame 병합 completed, 병합 후 size: {result_df.shape}") return result_df except Exception as e: logger.error(f"DataFrame 병합 중 error occurred: {e}") raise def normalize_column_names(self, df: pd.DataFrame) -> pd.DataFrame: """ columns name 표준화. Parameters ---------- df: pd.DataFrame process할 DataFrame Returns ------- pd.DataFrame columns name 표준화된 DataFrame """ if df.empty: return df # 원본 data 복사 processed_df = df.copy() # columns name convert rename_dict = {} for col in processed_df.columns: # 공백 언더스코어 변경 new_col = col.strip().replace(' ', '_') # 소문자 convert new_col = new_col.lower() # 특number문자 제거 (언더스코어 제외) new_col = ''.join(c for c in new_col if c.isalnum() or c == '_') # 중복된 언더스코어 제거 while '__' in new_col: new_col = new_col.replace('__', '_') # start/끝 언더스코어 제거 new_col = new_col.strip('_') if col != new_col: rename_dict[col] = new_col # columns name 변경 if rename_dict: processed_df = processed_df.rename(columns=rename_dict) logger.info(f"columns name {len(rename_dict)}개 표준화 completed") return processed_df def convert_data_types(self, df: pd.DataFrame, type_map: Optional[Dict[str, str]] = None) -> pd.DataFrame: """ data type convert. Parameters ---------- df: pd.DataFrame process할 DataFrame type_map: Dict[str, str], optional columns nameand data type 매핑 (예: {'column1': 'int', 'column2': 'float'}) Returns ------- pd.DataFrame data type convert된 DataFrame """ if df.empty: return df # 원본 data 복사 processed_df = df.copy() # type 매핑 없으면 자동 감지 if type_map is None: type_map = {} # number 보 string columns 감지 for col in processed_df.select_dtypes(include=['object']).columns: # 첫 100개 rows 샘플링 (or 전체 rows 100개 미만인 case) sample = processed_df[col].dropna().head(min(100, len(processed_df))) if sample.empty: continue # number convert available한지 check if all(isinstance(x, str) and x.replace('.', '', 1).isdigit() for x in sample if pd.notna(x) and isinstance(x, str)): # 소number점 있지 check if any('.' in str(x) for x in sample if pd.notna(x)): type_map[col] = 'float' else: type_map[col] = 'int' # data type convert for col, dtype in type_map.items(): if col in processed_df.columns: try: if dtype == 'int': processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0).astype(int) elif dtype == 'float': processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce') elif dtype == 'str' or dtype == 'string': processed_df[col] = processed_df[col].astype(str) elif dtype == 'bool' or dtype == 'boolean': processed_df[col] = processed_df[col].astype(bool) elif dtype == 'date' or dtype == 'datetime': processed_df[col] = pd.to_datetime(processed_df[col], errors='coerce') else: processed_df[col] = processed_df[col].astype(dtype) logger.info(f"columns '{col}' data type '{dtype}' convert completed") except Exception as e: logger.warning(f"columns '{col}' data type convert failed: {e}") return processed_df def calculate_derived_metrics(self, df: pd.DataFrame, metrics_config: Optional[Dict[str, Dict]] = None) -> pd.DataFrame: """ 파생 지표 계산. Parameters ---------- df: pd.DataFrame process할 DataFrame metrics_config: Dict[str, Dict], optional 파생 지표 configuration Returns ------- pd.DataFrame 파생 지표 add된 DataFrame """ if df.empty: return df # 원본 data 복사 processed_df = df.copy() # 기본 파생 지표 configuration default_metrics = { 'ctr': { 'formula': lambda x: (x['clicks'] / x['impressions']) * 100 if 'clicks' in x and 'impressions' in x else None, 'required_cols': ['clicks', 'impressions'], 'description': 'Click-Through Rate (%)' }, 'conversion_rate': { 'formula': lambda x: (x['conversions'] / x['clicks']) * 100 if 'conversions' in x and 'clicks' in x else None, 'required_cols': ['conversions', 'clicks'], 'description': 'Conversion Rate (%)' }, 'cost_per_click': { 'formula': lambda x: x['cost'] / x['clicks'] if 'cost' in x and 'clicks' in x and x['clicks'] > 0 else None, 'required_cols': ['cost', 'clicks'], 'description': 'Cost per Click' }, 'cost_per_conversion': { 'formula': lambda x: x['cost'] / x['conversions'] if 'cost' in x and 'conversions' in x and x['conversions'] > 0 else None, 'required_cols': ['cost', 'conversions'], 'description': 'Cost per Conversion' } } # usage자 정 지표 있으면 병합 if metrics_config: default_metrics.update(metrics_config) # 파생 지표 계산 for metric_name, config in default_metrics.items(): # needed한 columns 모두 있지 check required_cols = config.get('required_cols', []) if all(col in processed_df.columns for col in required_cols): try: # 지표 계산 formula = config['formula'] processed_df[metric_name] = processed_df.apply(formula, axis=1) logger.info(f"파생 지표 '{metric_name}' 계산 completed: {config.get('description', '')}") except Exception as e: logger.warning(f"파생 지표 '{metric_name}' 계산 failed: {e}") return processed_df def process_pipeline(self, df: pd.DataFrame) -> pd.DataFrame: """ 전체 data process 파프라인 execute. Parameters ---------- df: pd.DataFrame process할 원본 DataFrame Returns ------- pd.DataFrame 완전히 process된 DataFrame """ if df.empty: logger.warning("process할 data not available.") return df logger.info(f"data process 파프라인 start, 원본 data size: {df.shape}") # 1. columns name 표준화 processed_df = self.normalize_column_names(df) # 2. data 정제 processed_df = self.clean_dataframe(processed_df) # 3. data type convert processed_df = self.convert_data_types(processed_df) # 4. date format 표준화 processed_df = self.standardize_date_format(processed_df) # 5. 파생 지표 계산 processed_df = self.calculate_derived_metrics(processed_df) logger.info(f"data process 파프라인 completed, process 후 data size: {processed_df.shape}") return processed_df def save_processed_data(self, df: pd.DataFrame, output_path: str = "data/processed/") -> str: """ process된 DataFrame save. Parameters ---------- df: pd.DataFrame save할 DataFrame output_path: str output directory 경 Returns ------- str save된 file 경 """ if df.empty: logger.warning("save할 data not available.") return "" # directory 없으면 create os.makedirs(output_path, exist_ok=True) # current date file명 add current_date = datetime.now().strftime("%Y%m%d") file_path = os.path.join(output_path, f"processed_data_{current_date}.csv") # CSV save df.to_csv(file_path, index=False) logger.info(f"process된 data CSV save completed: {file_path}") return file_path # usage example if __name__ == "__main__": # 깅 configuration logging.basicConfig( level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' ) # data 프세서 create processor = DataProcessor() # 테스트 data create test_data = { 'Date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'], 'Source': ['Google Ads', 'Google Ads', 'Meta Ads', 'Meta Ads', 'Google Ads'], 'Impressions': [1000, 1200, 800, 950, 1100], 'Clicks': [50, 60, 40, 45, 55], 'Cost': [100.5, 120.25, 80.75, 95.5, 110.25], 'Conversions': [5, 6, 4, 4, 5] } df = pd.DataFrame(test_data) try: # data process 파프라인 execute processed_df = processor.process_pipeline(df) # result output print("\n원본 data:") print(df.head()) print("\nprocess된 data:") print(processed_df.head()) # CSV save file_path = processor.save_processed_data(processed_df) if file_path: print(f"\nCSV file save location: {file_path}") except Exception as e: print(f"error occurred: {e}")