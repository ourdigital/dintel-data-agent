""" data collection module. from various data sources collect data and save to central storage. """ import os import logging import yaml import pandas as pd import importlib from typing import Dict, List, Optional, Union, Any from datetime import datetime from pathlib import Path # import internal modules from src.database.db_manager import DatabaseManager from src.data.connectors.google_analytics import GoogleAnalyticsConnector # import other connectors (uncomment or implement as needed) # from src.data.connectors.google_ads import GoogleAdsConnector # from src.data.connectors.meta_ads import MetaAdsConnector # from src.data.connectors.naver_ads import NaverAdsConnector # from src.data.connectors.kakao_ads import KakaoAdsConnector logger = logging.getLogger(__name__) class DataAcquisition: """다양한 sourcefrom class for collecting data.""" def __init__(self, credentials_path: str = "config/api_credentials.yaml", config_path: str = "config/pipeline_config.yaml", db_manager: Optional[DatabaseManager] = None): """ DataAcquisition initialize. Parameters ---------- credentials_path: str API authentication information file path config_path: str pipeline configuration file path db_manager: DatabaseManager, optional existing DB manager object """ self.credentials_path = credentials_path self.config_path = config_path self.config = self._load_config() # DB manager configuration if db_manager: self.db_manager = db_manager else: self.db_manager = DatabaseManager(config_path) # initialize connectors self.connectors = {} self._initialize_connectors() def _load_config(self) -> Dict[str, Any]: """ load configuration file. Returns ------- Dict[str, Any] dictionary containing configuration information """ try: with open(self.config_path, 'r', encoding='utf-8') as f: config = yaml.safe_load(f) return config except Exception as e: logger.error(f"configuration file load failed: {e}") raise def _initialize_connectors(self) -> None: """initialize all available data source connectors.""" # get source configuration sources_config = self.config['collection']['sources'] # initialize implemented connectors for source_name, source_config in sources_config.items(): if not source_config.get('enabled', False): logger.info(f"source '{source_name}' is disabled.") continue try: if source_name == 'google_analytics': self.connectors[source_name] = GoogleAnalyticsConnector( credentials_path=self.credentials_path, config_path=self.config_path ) logger.info(f"Google Analytics initialize connectorsd successfully") # initialize other source connectors (uncomment or implement as needed) """ elif source_name == 'google_ads': self.connectors[source_name] = GoogleAdsConnector( credentials_path=self.credentials_path, config_path=self.config_path ) logger.info(f"Google Ads initialize connectorsd successfully") elif source_name == 'meta_ads': self.connectors[source_name] = MetaAdsConnector( credentials_path=self.credentials_path, config_path=self.config_path ) logger.info(f"Meta Ads initialize connectorsd successfully") elif source_name == 'naver_ads': self.connectors[source_name] = NaverAdsConnector( credentials_path=self.credentials_path, config_path=self.config_path ) logger.info(f"Naver Ads initialize connectorsd successfully") elif source_name == 'kakao_ads': self.connectors[source_name] = KakaoAdsConnector( credentials_path=self.credentials_path, config_path=self.config_path ) logger.info(f"Kakao Ads initialize connectorsd successfully") """ except Exception as e: logger.error(f"source '{source_name}' connector initialization failed: {e}") def collect_data_from_source(self, source_name: str) -> pd.DataFrame: """ collect data from specified source. Parameters ---------- source_name: str data source name Returns ------- pd.DataFrame DataFrame containing collected data """ if source_name not in self.connectors: logger.warning(f"source '{source_name}' connector has not been initialized.") return pd.DataFrame() try: logger.info(f"'{source_name}' starting data collection from") connector = self.connectors[source_name] # call appropriate method for each connector type if source_name == 'google_analytics': data = connector.fetch_data() # add source information if not data.empty: data['source'] = 'Google Analytics' # handle other sources (uncomment or implement as needed) """ elif source_name == 'google_ads': data = connector.fetch_data() if not data.empty: data['source'] = 'Google Ads' elif source_name == 'meta_ads': data = connector.fetch_data() if not data.empty: data['source'] = 'Meta Ads' elif source_name == 'naver_ads': data = connector.fetch_data() if not data.empty: data['source'] = 'Naver Ads' elif source_name == 'kakao_ads': data = connector.fetch_data() if not data.empty: data['source'] = 'Kakao Ads' """ else: logger.warning(f"source '{source_name}'for data collection method 정되지 않았습니다.") data = pd.DataFrame() logger.info(f"'{source_name}'from data collection completed, size: {data.shape}") return data except Exception as e: logger.error(f"'{source_name}'from data collection failed: {e}") return pd.DataFrame() def collect_data_from_all_sources(self) -> Dict[str, pd.DataFrame]: """ all 활성화된 sourcefrom data collection. Returns ------- Dict[str, pd.DataFrame] source name 키, collection된 data value 하 dictionary """ collected_data = {} for source_name in self.connectors.keys(): data = self.collect_data_from_source(source_name) if not data.empty: collected_data[source_name] = data logger.info(f"all sourcefrom data collection completed, source number: {len(collected_data)}") return collected_data def save_to_database(self, data: pd.DataFrame, table_name: str, if_exists: str = 'replace') -> bool: """ DataFrame data베스 save. Parameters ---------- data: pd.DataFrame save할 DataFrame table_name: str table name if_exists: str, default='replace' table 존재하 case process method ('replace', 'append', 'fail') Returns ------- bool success 여부 """ if data.empty: logger.warning(f"save할 data not available: {table_name}") return False try: # DB save with self.db_manager: self.db_manager.dataframe_to_sql(data, table_name, if_exists, index=False) logger.info(f"data table '{table_name}' save success, size: {data.shape}") return True except Exception as e: logger.error(f"data table '{table_name}' save failed: {e}") return False def save_to_csv(self, data: pd.DataFrame, source_name: str) -> str: """ DataFrame CSV file save. Parameters ---------- data: pd.DataFrame save할 DataFrame source_name: str data source name Returns ------- str save된 file 경 """ if data.empty: logger.warning(f"save할 data not available: {source_name}") return "" try: # output directory create output_dir = f"data/raw/{source_name.lower()}/" os.makedirs(output_dir, exist_ok=True) # current date file명 add current_date = datetime.now().strftime("%Y%m%d") file_path = os.path.join(output_dir, f"{source_name.lower()}_data_{current_date}.csv") # CSV save data.to_csv(file_path, index=False) logger.info(f"'{source_name}' data CSV save completed: {file_path}") return file_path except Exception as e: logger.error(f"'{source_name}' data CSV save failed: {e}") return "" def create_source_tables(self) -> None: """ 각 sourcefor data베스 table create. """ try: with self.db_manager: # Raw data table self.db_manager.create_table_if_not_exists( "raw_data", """ CREATE TABLE IF NOT EXISTS raw_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, date TEXT, source TEXT, data_json TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) """ ) # 각 source별 table (needed한 case) self.db_manager.create_table_if_not_exists( "google_analytics_data", """ CREATE TABLE IF NOT EXISTS google_analytics_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, date TEXT, sessions INTEGER, pageviews INTEGER, bounce_rate REAL, avg_session_duration REAL, source TEXT DEFAULT 'Google Analytics', created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) """ ) # process된 data table self.db_manager.create_table_if_not_exists( "processed_data", """ CREATE TABLE IF NOT EXISTS processed_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, date TEXT, source TEXT, campaign TEXT, impressions INTEGER, clicks INTEGER, conversions INTEGER, cost REAL, ctr REAL, conversion_rate REAL, cost_per_click REAL, cost_per_conversion REAL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) """ ) logger.info("data베스 table create completed") except Exception as e: logger.error(f"data베스 table create failed: {e}") def run_collection_pipeline(self, sources: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]: """ data collection 파프라인 execute. Parameters ---------- sources: List[str], optional collection할 source 목록. None면 all 활성화된 source. Returns ------- Dict[str, pd.DataFrame] source name 키, collection된 data value 하 dictionary """ # table create self.create_source_tables() # source 목록 check if sources is None: # all 활성화된 sourcefrom data collection collected_data = self.collect_data_from_all_sources() else: # 지정된 sourcefrom만 data collection collected_data = {} for source in sources: if source in self.connectors: data = self.collect_data_from_source(source) if not data.empty: collected_data[source] = data else: logger.warning(f"source '{source}' initialize되지 않았습니다.") # 각 source data save for source_name, data in collected_data.items(): # CSV file save self.save_to_csv(data, source_name) # data베스 save table_name = f"{source_name.lower()}_data" self.save_to_database(data, table_name) logger.info(f"data collection 파프라인 completed, process된 source number: {len(collected_data)}") return collected_data # usage example if __name__ == "__main__": # 깅 configuration logging.basicConfig( level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' ) # data collection object create acquisition = DataAcquisition() try: # 단days sourcefrom data collection source_name = 'google_analytics' print(f"\n{source_name}from data collection 중...") data = acquisition.collect_data_from_source(source_name) if not data.empty: print(f"data collection success! size: {data.shape}") print("\ndata preview:") print(data.head()) # CSV save file_path = acquisition.save_to_csv(data, source_name) if file_path: print(f"\nCSV file save location: {file_path}") # DB save table_name = f"{source_name.lower()}_data" success = acquisition.save_to_database(data, table_name) if success: print(f"\ndata table '{table_name}' save되었습니다.") else: print("data collection failed or data not available.") # 전체 파프라인 execute print("\n전체 data collection 파프라인 execute 중...") collected_data = acquisition.run_collection_pipeline() print(f"\n파프라인 completed! collection된 source number: {len(collected_data)}") for source, df in collected_data.items(): print(f"- {source}: {df.shape[0]}rows x {df.shape[1]}columns") except Exception as e: print(f"error occurred: {e}")